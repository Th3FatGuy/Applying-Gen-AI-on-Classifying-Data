{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e775bd91",
   "metadata": {},
   "source": [
    "# Applying Google AI on Classifying Data from Job Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa6846",
   "metadata": {},
   "source": [
    "## Case scenerio:\n",
    "There are numerous course alternatives available to students within the same major before they enroll in a university, academy, or college. There is a lack of specialized knowledge and skills in real-life jobs, however, resulting from the majority of these courses' shallow instruction, which makes it challenging for students to obtain them. On the other hand, a lack of highly qualified workers for many organizations in emerging nations makes hiring difficult, particularly for senior roles that still need more talent. By using Google Gen AI to categorize data and skill sets, this initiative hopes to assist educational institutions in identifying and improving the specialized skills and tools required in their curricula so that students are better equipped for the workforce.\n",
    "\n",
    "üìÑ **Use case of project**:\n",
    "- Using Selenium with Beutiful Soup to collect data from career website\n",
    "- Classifying different categories from job post using Google Generative AI\n",
    "\n",
    "ü§ñ **Gen AI capabilities**:\n",
    "- Document understanding: Understanding different job post\n",
    "- Structured output/JSON mode: Return result as JSON\n",
    "- Few-shot prompting: Classifying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install pandas selenium tldextract beutyfulsoup4 requests\n",
    "# %pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c772a2",
   "metadata": {},
   "source": [
    "Callout libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360a1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from google import genai\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    ")\n",
    "\n",
    "import tldextract\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import json\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.utils import get_column_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce3a79",
   "metadata": {},
   "source": [
    "Read config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "572c368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConfigFile.properties']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read('ConfigFile.properties')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6062542",
   "metadata": {},
   "source": [
    "Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c31ff439",
   "metadata": {},
   "outputs": [],
   "source": [
    "today =  datetime.today().strftime('%Y-%m-%d')\n",
    "company_url = config.get('URL', 'url.broadcom')\n",
    "googleai_key = config.get('API', 'api.key_googleai')\n",
    "company = \"Broadcom\"\n",
    "webdriver_service = Service(config.get('URL', 'url.chromedriver')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a29529",
   "metadata": {},
   "source": [
    "Define functions:\n",
    "- **collect_job_links()**: function to collect career links from Broadcom career website\n",
    "- **convert_posted_date()**: the date on career website are being set as: *\"Posted Today\", \"Posted Yesterday\", \"Posted 2 days ago\", etc.*, so the function will converted to date as type ***%d-%m-&Y***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8424fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_job_links(driver):\n",
    "    job_urls = []\n",
    "    while True:\n",
    "        try:\n",
    "            job_links = driver.find_elements(By.XPATH, \"//a[@data-automation-id='jobTitle']\")\n",
    "            job_urls.extend([link.get_attribute(\"href\") for link in job_links])\n",
    "            \n",
    "            next_button = driver.find_element(By.XPATH, \"//button[@data-uxi-element-id='next']\")\n",
    "            \n",
    "            if not next_button.is_enabled():\n",
    "                break\n",
    "            \n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            print(\"No more pages or next button not found, stopping...\")\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout while loading page, stopping...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return job_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3624a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_posted_date(posted_str):\n",
    "    today = datetime.today()\n",
    "    \n",
    "    if 'Today' in posted_str:\n",
    "        return today.strftime(\"%d-%m-%Y\")\n",
    "    elif 'Yesterday' in posted_str:\n",
    "        return (today - timedelta(days=1)).strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    match = re.search(r'(\\d+)', posted_str)\n",
    "    if match:\n",
    "        days_ago = int(match.group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d1a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m driver = webdriver.Chrome(service=webdriver_service)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m      3\u001b[39m company = tldextract.extract(company_url).domain\n\u001b[32m      4\u001b[39m driver.get(company_url)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "time.sleep(5)  \n",
    "company = tldextract.extract(company_url).domain\n",
    "driver.get(company_url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "try:\n",
    "    cookie_banner = driver.find_element(By.ID, \"onetrust-policy\")\n",
    "    dismiss_button = cookie_banner.find_element(By.XPATH, \"//button[contains(text(), 'Accept') or contains(text(), 'I Understand')]\") # Change the text if needed\n",
    "    dismiss_button.click()  # Accept cookies to remove the banner\n",
    "except NoSuchElementException:\n",
    "    print(\"No cookie banner found, proceeding...\")\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    job_links = collect_job_links(driver)\n",
    "except NoSuchElementException:\n",
    "    print(\"No job links found, proceeding...\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links_df = pd.DataFrame(job_links, columns=[\"Job Links\"])\n",
    "# For backup purposes, save the job links to a CSV file\n",
    "job_links_df.to_csv(\"Broadcom_job_links.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case that the ipynb restarts from the beginning, uncomment the following line to read the job links from the CSV file\n",
    "# job_links_df = pd.read_csv(\"Broadcom_job_links.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8946f48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     65\u001b[39m             \u001b[38;5;66;03m# To avoid overwhelming the server, wait a bit before the next request\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m             \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m driver3.quit()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_job_data = []\n",
    "# For test case extract first 10 job links\n",
    "job_links_test = job_links_df['Job Links'][:10]\n",
    "driver3 = webdriver.Chrome(service=webdriver_service)\n",
    "with httpx.Client(timeout=10) as client:\n",
    "    for link in job_links_df['Job Links']:\n",
    "        driver3.get(link)\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            soup = BeautifulSoup(driver3.page_source, 'html.parser')\n",
    "            time.sleep(2)\n",
    "            job_metadata = {}\n",
    "            \n",
    "            job_title_tag = soup.find(\"h2\", {\"data-automation-id\": \"jobPostingHeader\"})\n",
    "            job_title = job_title_tag.get_text(strip=True) if job_title_tag else \"N/A\"\n",
    "\n",
    "            job_location_div = soup.find(\"div\", {\"data-automation-id\": \"locations\"})\n",
    "            job_locations = []\n",
    "            if job_location_div:\n",
    "                locations = job_location_div.find_all(\"dd\", class_=\"css-129m7dg\")\n",
    "                job_locations = [loc.get_text(strip=True) for loc in locations]\n",
    "            job_location = \"\\n \".join(job_locations) if job_locations else \"N/A\"\n",
    "\n",
    "            date_posted_div = soup.find(\"div\", {\"data-automation-id\": \"postedOn\"})\n",
    "            date_posted = \"N/A\"\n",
    "            if date_posted_div:\n",
    "                date_tag = date_posted_div.find(\"dd\", class_=\"css-129m7dg\")\n",
    "                date_posted = date_tag.get_text(strip=True) if date_tag else \"N/A\"\n",
    "            date_posted = convert_posted_date(date_posted)\n",
    "\n",
    "            job_id_div = soup.find(\"div\", {\"data-automation-id\": \"requisitionId\"})\n",
    "            job_id = \"N/A\"\n",
    "            if job_id_div:\n",
    "                job_id_tag = job_id_div.find(\"dd\", class_=\"css-129m7dg\")\n",
    "                job_id = job_id_tag.get_text(strip=True) if job_id_tag else \"N/A\"\n",
    "\n",
    "        \n",
    "            job_type_div = soup.find(\"div\", {\"data-automation-id\": \"time\"})\n",
    "            job_type = job_type_div.get_text(strip=True) if job_type_div else \"N/A\"\n",
    "            if job_type != \"N/A\":\n",
    "                job_type = job_type[9::]\n",
    "\n",
    "            job_description_elem = WebDriverWait(driver3, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-automation-id='jobPostingDescription']\"))\n",
    "            )\n",
    "            job_text = job_description_elem.find_elements(By.CSS_SELECTOR, \"p, ul, ul li\")\n",
    "            job_description = \"\\n\".join([p.text for p in job_text])\n",
    "\n",
    "            job_data = {\n",
    "                \"Company\": company,\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": job_location,\n",
    "                \"Date Posted\": date_posted,\n",
    "                \"Employment Type\": job_type,\n",
    "                \"Job ID\": job_id,\n",
    "                \"Job Context\": job_description,\n",
    "                \"Link\": link\n",
    "            }\n",
    "\n",
    "            all_job_data.append(job_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "        finally:\n",
    "            # To avoid overwhelming the server, wait a bit before the next request\n",
    "            time.sleep(15)\n",
    "\n",
    "driver3.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dict list into a DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Save data to CSV file for backup\n",
    "df.to_excel(\"Broadcom_job_data.xlsx\", index=False)\n",
    "\n",
    "# Preview the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The promt is in Vietnamese, howeverm the response is in English.\n",
    "# If you want to change the prompt to English, please change the DEFINE_PROMPT variable below.\n",
    "DEFINE_PROMPT = \"\"\"\n",
    "T·ª´ ƒëo·∫°n text sau, b·∫°n h√£y ph√¢n t√≠ch gi√∫p t√¥i v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi sau, k·∫øt qu·∫£ tr·∫£ v·ªÅ cho c√°c variable t∆∞∆°ng ·ª©ng nh∆∞ sau:\n",
    "- work_category: Danh m·ª•c c√¥ng vi·ªác n√†y l√† g√¨? (VD: Software Engineer, Data Analyst, ...)\n",
    "- domain: ph√¢n lo·∫°i m·∫£ng l√†m vi·ªác ch√≠nh (v√≠ d·ª•: IC Design, Software, Backend, Frontend, Embedded, DevOps, Firmware, System, v.v.) \n",
    "- job_description: M√¥ t·∫£ c√¥ng vi·ªác n√†y l√† c·∫ßn l√†m nh·ªØng vi·ªác g√¨?\n",
    "- job_responsibility: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng tr√°ch nhi·ªám g√¨?\n",
    "- job_requirements: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng y√™u c·∫ßu g√¨?\n",
    "- require_tools: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng k·ªπ nƒÉng v·ªÅ c√¥ng c·ª• n√†o?\n",
    "- require_certificate: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng b·∫±ng c·∫•p g√¨? (Kh√¥ng t√≠nh b·∫±ng ƒë·∫°i h·ªçc nh∆∞ master, bachelor, ...)\n",
    "- require_skills: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng k·ªπ nƒÉng g√¨?\n",
    "- job_experience: C√¥ng vi·ªác n√†y c·∫ßn nh·ªØng kinh nghi·ªám g√¨?\n",
    "- education_level: B·∫±ng c·∫•p h·ªçc v·∫•n c·∫ßn thi·∫øt cho c√¥ng vi·ªác n√†y l√† g√¨? (VD: Bachelor's, Master's, PhD, ...)\n",
    "- education_major: Chuy√™n ng√†nh h·ªçc c·∫ßn thi·∫øt cho c√¥ng vi·ªác n√†y l√† g√¨? (VD: Computer Science, Logistic, Engineering..., n·∫øu kh√¥ng ƒë∆∞·ª£c n√™u trong m√¥ t·∫£ th√¨ d·ª± ƒëo√°n nh·ªØng chuy√™n ng√†nh c√≥ li√™n quan nh·∫•t t·ªõi ng√†nh ngh·ªÅ n√†y gi√∫p m√¨nh nha)\n",
    "----\n",
    "text: {job_description}\n",
    "----\n",
    "Tr·∫£ v·ªÅ th√¥ng tin theo ƒë·ªãnh d·∫°ng JSON nh∆∞ sau:\n",
    "{{\n",
    "    \"job_description\": \"\",\n",
    "    \"job_responsibility\": \"\",\n",
    "    \"job_requirements\": \"\",\n",
    "    \"require_tools\": [],\n",
    "    \"require_skills\": [],\n",
    "    \"require_certificate\": []\n",
    "    \"job_experience\": \"\",\n",
    "    \"education_level\": \"\",\n",
    "    \"education_major\": \"\"\n",
    "}}\n",
    "Ch·ªâ tr·∫£ v·ªÅ th√¥ng tin theo d·∫°ng tr√™n theo ti·∫øng Anh, kh√¥ng th√™m ph·∫ßn gi·ªõi thi·ªáu hay k·∫øt lu·∫≠n n√†o kh√°c. Ngo√†i ra ƒë·ª´ng tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng markdown.\n",
    "Ngo√†i ra, t·∫•t ƒë·ªëi v·ªõi require_tools, require_skills, require_certificate, education_level, education_major th√¨ t·∫•t c·∫£ kh√¥ng vi·∫øt hoa.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6445b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=googleai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirement(job_description):\n",
    "    chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    prompt = DEFINE_PROMPT.format(job_description=job_description)\n",
    "    \n",
    "    response = chat.send_message(\n",
    "        message=prompt,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_parse_json(raw_response):\n",
    "    cleaned = raw_response.strip()\n",
    "\n",
    "    # Remove markdown (```json ... ```)\n",
    "    cleaned = re.sub(r\"^```json\\s*|\\s*```$\", \"\", cleaned, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Replace None (Python) to null (JSON)\n",
    "    cleaned = cleaned.replace(\"None\", \"null\")\n",
    "\n",
    "    # if \"null\" return as string then return null as value\n",
    "    cleaned = re.sub(r'\"\\s*null\\s*\"', 'null', cleaned)\n",
    "\n",
    "    # Sometime it return both version in English and Vietnamese, so we need to remove the Vietnamese version\n",
    "    if cleaned.count(\"{\") > 1:\n",
    "        cleaned = cleaned.split(\"}\", 1)[0] + \"}\"\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSONDecodeError:\", e)\n",
    "        print(\"Error string:\", repr(cleaned))\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_category = []\n",
    "domain = []\n",
    "job_descriptions = []\n",
    "job_responsibilities = []\n",
    "job_requirements = []\n",
    "tools = []\n",
    "skills = []\n",
    "certs = []\n",
    "job_experience = []\n",
    "education_level = []\n",
    "education_major = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        result = extract_requirement(row[\"Job Context\"])\n",
    "        parsed = clean_and_parse_json(result)\n",
    "\n",
    "        work_category.append(parsed.get(\"work_category\", \"\"))\n",
    "        domain.append(parsed.get(\"domain\", \"\"))\n",
    "        job_descriptions.append(parsed.get(\"job_description\", \"\"))\n",
    "        job_responsibilities.append(parsed.get(\"job_responsibility\", \"\"))\n",
    "        job_requirements.append(parsed.get(\"job_requirements\", \"\"))\n",
    "        tools.append(\", \".join(x for x in parsed.get(\"require_tools\") or [] if x))\n",
    "        skills.append(\", \".join(x for x in parsed.get(\"require_skills\") or [] if x))\n",
    "        certs.append(\", \".join(x for x in parsed.get(\"require_certificate\") or [] if x))\n",
    "        job_experience.append(parsed.get(\"job_experience\", \"\"))\n",
    "        education_level.append(parsed.get(\"eduction_level\", \"\"))\n",
    "        education_major.append(parsed.get(\"eduction_major\", \"\"))\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "\n",
    "    # Due to the rpm limitation of Google AI, we need to sleep for 20 seconds after each request\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Work Category\"] = work_category\n",
    "df[\"Domain\"] = domain\n",
    "df[\"Job Description New\"] = job_descriptions\n",
    "df[\"Job Responsibility\"] = job_responsibilities\n",
    "df[\"Job Requirements\"] = job_requirements\n",
    "df[\"Job Experience\"] = job_experience\n",
    "df[\"Education Level\"] = education_level\n",
    "df[\"Education Major\"] = education_major\n",
    "df[\"Require Tools\"] = tools\n",
    "df[\"Require Skills\"] = skills\n",
    "df[\"Require Certificate\"] = certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV file for backup\n",
    "df.to_csv(\"Broadcom_job_data_v2.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9c4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set width of column A (header 'Company') to 30\n",
      "Set width of column B (header 'Job Title') to 50\n",
      "Set width of column C (header 'Job Location') to 30\n",
      "Set width of column D (header 'Date Posted') to 30\n",
      "Set width of column E (header 'Employment Type') to 30\n",
      "Set width of column F (header 'Job ID') to 30\n",
      "Set width of column G (header 'Job Description') to 50\n",
      "Set width of column H (header 'Job Context') to 50\n",
      "Set width of column I (header 'Job Description New') to 50\n",
      "Set width of column J (header 'Job Responsibility') to 50\n",
      "Set width of column K (header 'Job Requirements') to 50\n",
      "Set width of column L (header 'Job Experience') to 15\n",
      "Set width of column M (header 'Education Level') to 15\n",
      "Set width of column N (header 'Education Major') to 15\n",
      "Set width of column O (header 'Require Tools') to 25\n",
      "Set width of column P (header 'Require Skills') to 25\n",
      "Set width of column Q (header 'Require Certificate') to 25\n",
      "Set width of column R (header 'Work Category') to 20\n",
      "Set width of column S (header 'Domain') to 20\n"
     ]
    }
   ],
   "source": [
    "wb = load_workbook(\"Broadcom_job_data_v2.xlsx\")\n",
    "ws = wb.active\n",
    "today = time.strftime(\"%Y-%m-%d\")\n",
    "company = \"Broadcom\"\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a ƒë·ªô r·ªông c·ªôt\n",
    "column_settings = {\n",
    "    (\"Job Title\", \"Job Description\", \"Job Description New\", \"Job Responsibility\", \"Job Requirements\", \"Job Context\"): 50,\n",
    "    (\"Company\", \"Job Location\", \"Date Posted\", \"Employment Type\", \"Job ID\"): 30,\n",
    "    (\"Work Category\", \"Domain\"): 20,\n",
    "    (\"Require Tools\", \"Require Skills\", \"Require Certificate\"): 25,\n",
    "    (\"Job Experience\", \"Education Level\", \"Education Major\"): 15,\n",
    "    (\"Link\",): 50,\n",
    "}\n",
    "\n",
    "# Thi·∫øt l·∫≠p ƒë·ªô r·ªông c·ªôt v√† b·∫≠t wrap_text\n",
    "for cell in ws[1]:  # L·∫∑p qua h√†ng ƒë·∫ßu ti√™n (header)\n",
    "    if cell.value:\n",
    "        header = cell.value.strip()\n",
    "        for headers, width in column_settings.items():\n",
    "            if header in headers:\n",
    "                col_letter = get_column_letter(cell.column)\n",
    "                ws.column_dimensions[col_letter].width = width\n",
    "                print(f\"Set width of column {col_letter} (header '{header}') to {width}\")\n",
    "\n",
    "                # B·∫≠t wrap_text cho t·∫•t c·∫£ c√°c √¥ trong c·ªôt\n",
    "                for row in ws.iter_rows(min_col=cell.column, max_col=cell.column, min_row=2, max_row=ws.max_row):\n",
    "                    for c in row:\n",
    "                        c.alignment = Alignment(wrap_text=True)\n",
    "\n",
    "# CƒÉn ch·ªânh to√†n b·ªô sheet\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        if cell.alignment:\n",
    "            cell.alignment = Alignment(\n",
    "                horizontal=cell.alignment.horizontal or \"left\",\n",
    "                vertical=\"top\",\n",
    "                text_rotation=cell.alignment.text_rotation,\n",
    "                wrap_text=cell.alignment.wrap_text,\n",
    "                shrink_to_fit=cell.alignment.shrink_to_fit,\n",
    "                indent=cell.alignment.indent\n",
    "            )\n",
    "\n",
    "# H√†m t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh chi·ªÅu cao h√†ng d·ª±a tr√™n wrap_text\n",
    "def auto_adjust_row_heights(worksheet, base_height=15, words_per_line=7):\n",
    "    for row in worksheet.iter_rows(min_row=1, max_row=worksheet.max_row):\n",
    "        max_lines = 1\n",
    "        for cell in row:\n",
    "            if cell.alignment and cell.alignment.wrap_text and cell.value:\n",
    "                lines = str(cell.value).split('\\n')\n",
    "                estimated_lines = sum((len(line.split()) + words_per_line - 1) // words_per_line for line in lines)\n",
    "                max_lines = max(max_lines, estimated_lines)\n",
    "        worksheet.row_dimensions[row[0].row].height = base_height * max_lines\n",
    "\n",
    "# ƒêi·ªÅu ch·ªânh chi·ªÅu cao h√†ng\n",
    "auto_adjust_row_heights(ws)\n",
    "\n",
    "# L∆∞u file Excel m·ªõi\n",
    "output_file = f\"./{company}_job_posts_allcountries_raw_{today}.xlsx\"\n",
    "wb.save(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
